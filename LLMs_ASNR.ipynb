{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahriar-faghani/ASNR_ASFNR_AI_Workshop_2024/blob/main/LLMs_ASNR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEyKARVcnrUZ"
      },
      "source": [
        "# **Large Language Models: Zero-Shot Learning, Few-Shot Learning and, RAG**\n",
        "\n",
        "---\n",
        "\n",
        "Radiology Informatics Lab, Department of Radiology, Mayo Clinic (MN):\n",
        "\n",
        "<b>\n",
        "Shahriar Faghani, MD\n",
        "</b>\n",
        "\n",
        "---\n",
        "\n",
        "In recent years, the applications of large language models (LLMs) like GPT-4 have expanded at an exponential pace. However, like all tools, LLMs come with their set of limitations. One of the prominent challenges is the **\"hallucination\"** errors, where the model might generate information that is incorrect or not present in its training data. In fields like medicine, such errors could lead to misleading interpretations and, in worst-case scenarios, detrimental patient outcomes.\n",
        "\n",
        "In this notebook we will learn about **Retrieval Augmented Generation (RAG)**, an approach that may help mitigate the hallcuination errors in LLMs. This approach synergizes the powerful generative capabilities of LLMs with the accuracy of retrieval-based models. In RAG, when a query is made, the model first fetches relevant documents or data snippets (retrieval phase) from a large pool of documents (could be already available or also provided by the user) and then uses this information to generate a response (generation phase). By combining the strengths of both retrieval and generation models, RAG aims to provide more accurate and contextually relevant answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv4Ejg7nChQ8"
      },
      "source": [
        "## **Part 0: Setting the scene**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkmz5IrNn1Qu"
      },
      "source": [
        "### Setting Up the Environment\n",
        "\n",
        "Before diving into Retrieval Augmented Generation (RAG), we need to set up our environment by installing the necessary libraries. The libraries listed here provide us with tools and functionalities to implement and leverage RAG, as well as other related processes. Here's a brief overview of some of the core libraries:\n",
        "\n",
        "*   **transformers**: Contains implementations of many state-of-the-art models, including those related to RAG.\n",
        "*   **sentence-transformers**: Helps in creating embeddings for sentences, useful for the retrieval phase in RAG.\n",
        "*   **chromadb**: Facilitates interactions with databases and external data sources.\n",
        "*   **accelerate**: Aids in accelerating Python workflows.\n",
        "*   **einops** and **xformers**: Offer advanced operations and architectures for neural networks.\n",
        "*   **bitsandbytes**: Assists in efficient deep learning model loading.\n",
        "*   **pypdf** and **pymupdf**: Assist in parsing the PDF files.\n",
        "\n",
        "After installing these, we can import the necessary modules to prepare for our subsequent RAG experiments.\n",
        "\n",
        "----\n",
        "> **Note**: You do not need any token or API keys for running this notebook. In the later cells, we will run a few tasks using the OpenAI models, but the outputs of those cells are precomputed and already avialable to you.\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAAOzCbaz_i9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d53deef-62ab-4024-8c0a-6cc7f22723d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.7/222.7 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.8/526.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install new packages\n",
        "!pip install -qU --no-warn-conflicts \\\n",
        "  transformers==4.40.2 \\\n",
        "  sentence-transformers==2.7.0 \\\n",
        "  accelerate==0.30.1 \\\n",
        "  einops==0.8.0 \\\n",
        "  xformers==0.0.26.post1 \\\n",
        "  bitsandbytes==0.43.1\\\n",
        "  chromadb==0.5.0\\\n",
        "  pypdf==4.2.0\\\n",
        "  pymupdf==1.24.4 \\\n",
        "  torch==2.3.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBYTpJ8g_VO3"
      },
      "source": [
        "### Environment Configuration\n",
        "\n",
        "In this section, we're setting up some preliminary configurations to ensure our experiments run seamlessly."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLMs process inputs and outputs in chunks called tokens. Think of these, roughly, as words – each model will have its own tokenization scheme. For example, this sentence...\n",
        "\n",
        "Our destiny is written in the stars.\n",
        "\n",
        "...is tokenized into [\"Our\", \" destiny\", \" is\", \" written\", \" in\", \" the\", \" stars\", \".\"] for Llama 3. See [this](https://tiktokenizer.vercel.app/?model=meta-llama%2FMeta-Llama-3-8B) for an interactive tokenizer tool."
      ],
      "metadata": {
        "id": "fWFlmvlmKwgk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzSIVL93rOC0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Configure the display ooptions\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.options.display.max_colwidth = 1000\n",
        "\n",
        "# Remove the sample_data directory by Google Colab\n",
        "if os.path.exists('sample_data'):\n",
        "  shutil.rmtree('sample_data')\n",
        "\n",
        "# Set the seed for the random libraries\n",
        "random.seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWmVlj46n3uj"
      },
      "source": [
        "### Data Acquisition and Preparation\n",
        "\n",
        "To explore and validate our RAG model, we utilized a dataset comprising 4 open-access articles from *American Journal of Neuroradiology* (*AJNR*).\n",
        "\n",
        "In the cells below, we will donwload these PDF files page by page and explore it a little bit..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the file from GitHub\n",
        "!wget -q -O ASNR_ASFNR_AI_Workshop.zip https://github.com/shahriar-faghani/ASNR_ASFNR_AI_Workshop_2024/raw/main/ASNR_ASFNR_AI_Workshop.zip\n",
        "\n",
        "# Unzip the file to extract only the needed structure\n",
        "!unzip -q ASNR_ASFNR_AI_Workshop.zip -d .\n",
        "\n",
        "# Remove the extra directory and the zip file\n",
        "!rm -r ASNR_ASFNR_AI_Workshop.zip"
      ],
      "metadata": {
        "id": "PGZFhgMuUDHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FJrg_wz5MSh"
      },
      "outputs": [],
      "source": [
        "root = '/content/ASNR_ASFNR_AI_Workshop'\n",
        "articles_dir = os.path.join(root, 'Articles')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8H-hHeZHMGq"
      },
      "outputs": [],
      "source": [
        "# Load the PDF files for all articles and parse them page by page\n",
        "from pypdf import PdfReader\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "pdf_paths = [os.path.join(articles_dir, file) for file in os.listdir(articles_dir)]\n",
        "pdf_docs = list()\n",
        "for pdf_path in tqdm(pdf_paths, total=len(pdf_paths)):\n",
        "  reader = PdfReader(pdf_path)\n",
        "  for i, page in enumerate(reader.pages):\n",
        "    page_content = page.extract_text()\n",
        "    pdf_docs.append({\n",
        "        \"source\": pdf_path,\n",
        "        \"page\": i,\n",
        "        \"page_content\": page_content\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8vYgd_dZiHf"
      },
      "outputs": [],
      "source": [
        "# Some texts may contain illegal chars that may confuse the downstream LLMs or\n",
        "# cause trouble when saving the text to disk. Let's remove them.\n",
        "\n",
        "def remove_illegal_chars(text):\n",
        "    illegal_chars = [\n",
        "        '\\x00', '\\x01', '\\x02', '\\x03', '\\x04', '\\x05', '\\x06',\n",
        "        '\\x07', '\\x08', '\\x0b', '\\x0c', '\\x0e', '\\x0f', '\\x10',\n",
        "        '\\x11', '\\x12', '\\x13', '\\x14', '\\x15', '\\x16', '\\x17',\n",
        "        '\\x18', '\\x19', '\\x1a', '\\x1b', '\\x1c', '\\x1d', '\\x1e',\n",
        "        '\\x1f'\n",
        "    ]\n",
        "    for char in illegal_chars:\n",
        "        text = text.replace(char, '')\n",
        "    return text\n",
        "\n",
        "for pdf_doc in pdf_docs:\n",
        "  pdf_doc['page_content'] = remove_illegal_chars(pdf_doc['page_content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOTqjSRacMdP"
      },
      "outputs": [],
      "source": [
        "# Investigate the loaded PDF files\n",
        "\n",
        "print(f'Number of documents: {len(pdf_docs)}')\n",
        "\n",
        "# Show the loaded pages as a dataframe\n",
        "\n",
        "df = pd.DataFrame(pdf_docs)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDNH7DG7uJ7_"
      },
      "source": [
        "### Regular question-answering with Open Source LLMs\n",
        "\n",
        "Before diving into Retrieval Augmented Generation (RAG), it's crucial to understand the performance of traditional Large Language Models (LLMs) without retrieval augmentation. For this purpose, we're setting up a baseline using Llama 3, a state-of-the-art open-source language model.\n",
        "\n",
        "The provided code performs the following tasks:\n",
        "\n",
        "1.   Specifies the model_id corresponding to Llama 3 available on the HuggingFace Model Hub.\n",
        "2.   Determines the computational device (GPU or CPU) for running the model.\n",
        "3.   Configures quantization settings via BitsAndBytesConfig to load the model using reduced memory. Quantization is a technique to store and compute on model parameters using fewer bits, which can be particularly useful when working with large models on limited hardware.\n",
        "4.   Initializes the model configuration and the model itself using the provided model_id."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Special setup for Llama 3\n",
        "\n",
        "You need to request access from [here](https://llama.meta.com/llama-downloads/). Then you need to create a token within your huggingface account and use this token in the cell below."
      ],
      "metadata": {
        "id": "iFDTE7Y1PW2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=\"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\")"
      ],
      "metadata": {
        "id": "BOcnLRdpC5WQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gC2OaVVs6sb"
      },
      "outputs": [],
      "source": [
        "# Loading the model from HuggingFace\n",
        "\n",
        "from torch import cuda, bfloat16\n",
        "import transformers\n",
        "\n",
        "model_id = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
        "# model_id = 'mistralai/Mixtral-8x7B-Instruct-v0.1'\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# set quantization configuration to load large model with less GPU memory\n",
        "# this requires the `bitsandbytes` library\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")\n",
        "\n",
        "# begin initializing HF items, need auth token for these\n",
        "model_config = transformers.AutoConfig.from_pretrained(model_id)\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        ")\n",
        "model.eval()\n",
        "print(f\"Model loaded on {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFc7-xH1Ey_Y"
      },
      "source": [
        "A crucial step in working with language models is to convert the textual input into a format that the model can understand. This process is known as **tokenization**. Essentially, tokenization breaks down text into smaller pieces, commonly called tokens. These tokens are then mapped to unique integers, allowing them to be processed by the model. Please refer to this [tutorial](https://medium.com/@fhirfly/understanding-tokens-in-the-context-of-large-language-models-like-bert-and-t5-8aa0db90ef39) to learn more about tokenization.\n",
        "\n",
        "Let's also load a tokenizer from HuggingFace. We need to pass the `model_id` so that we load the appropriate tokenizer for our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRzTIL1jv1Br"
      },
      "outputs": [],
      "source": [
        "# Setup a tokenizer\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOuAC8PZfJad"
      },
      "source": [
        "In the next cell, we will define a function that you can use for question-answering with HuggingFace models, includingh the Llama 3 instruct model we defined earlier."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def qa_with_hf_llms(\n",
        "    messages,\n",
        "    model,\n",
        "    tokenizer,\n",
        "    temperature = 0.5, # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
        "    top_p = 0.9, # breadth of generated outputs\n",
        "    max_tokens = 2000, # max number of tokens to generate in the output\n",
        "):\n",
        "    terminators = [\n",
        "    tokenizer.eos_token_id,\n",
        "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=max_tokens,\n",
        "        eos_token_id=terminators,\n",
        "        do_sample=True,\n",
        "        temperature = temperature,\n",
        "        top_p = top_p,\n",
        "    )\n",
        "\n",
        "  # Infer\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "    response = tokenizer.decode(response, skip_special_tokens=True)\n",
        "    return response"
      ],
      "metadata": {
        "id": "L-bXZQG4RT7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to use other models like Mixtral use the cell below. (You need to comment the above cell and uncomment the below cell!!)"
      ],
      "metadata": {
        "id": "S6-1Nw9KSAMg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9rtymkmcjNE",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# def qa_with_hf_llms(\n",
        "#     prompt,\n",
        "#     model,\n",
        "#     tokenizer,\n",
        "#     tempreture=0.5,\n",
        "#     max_tokens=2000,\n",
        "#     frequency_penalty=0.0\n",
        "# ):\n",
        "\n",
        "#   # Build a HuggingFace generator on top of the HuggingFace model\n",
        "#   generator = transformers.pipeline(\n",
        "#       model=model,\n",
        "#       tokenizer=tokenizer,\n",
        "#       return_full_text=False,\n",
        "#       task='text-generation',\n",
        "#       temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
        "#       max_new_tokens=2000,  # max number of tokens to generate in the output\n",
        "#       repetition_penalty=1.1  # without this output begins repeating\n",
        "#   )\n",
        "\n",
        "#   # Necessary to enable batching for inference\n",
        "#   generator.tokenizer.pad_token_id = generator.model.config.eos_token_id\n",
        "\n",
        "#   # Infer\n",
        "#   res = generator(prompt, pad_token_id=tokenizer.eos_token_id)\n",
        "#   return res[0][\"generated_text\"].strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Report analysis:\n",
        "Let's start by analyzing this synthetic report!"
      ],
      "metadata": {
        "id": "8oKYDwonh4Xz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Provided with this report, determine if there is any metastasis. If yes, specify the vertebral level. Reason through each step and return the findings in a JSON format.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Technique: MRI of the thoracic and lumbar spine was performed using T1-weighted, T2-weighted, and STIR sequences in sagittal and axial planes. Findings:Alignment: Normal vertebral alignment is maintained. Bone Marrow Signal: There are areas in the L1 and L3 vertebral bodies that demonstrate altered signal characteristics, which might suggest metastatic involvement. These areas appear hypointense on T1-weighted images and hyperintense on STIR sequences. Disc Spaces: Intervertebral disc spaces are preserved with no significant disc herniation or bulging noted. Spinal Canal and Neural Foraminal: The spinal canal is of normal caliber with no evidence of significant stenosis. Neural foramina are patent bilaterally throughout the visualized levels. Cord and Conus: The spinal cord and conus medullaris demonstrate normal signal intensity with no focal lesions identified. Impression: Altered signal in L1 and L3 vertebral bodies: These findings could be suggestive of metastatic involvement. However, correlation with the patient's clinical history and additional imaging studies or biopsy may be warranted for further evaluation. No significant spinal canal or foraminal stenosis.Recommendations: Further evaluation with contrast-enhanced MRI or PET-CT may be considered to better characterize these findings. Clinical correlation and possibly a biopsy of the suspicious areas may be needed to confirm the presence of metastasis.\"},\n",
        "]\n",
        "\n",
        "llm_response = qa_with_hf_llms(messages, model, tokenizer)\n",
        "\n",
        "print(f'Prompt:\\n```{messages}```')\n",
        "print(f'\\nLLM response:\\n```{llm_response}```')"
      ],
      "metadata": {
        "id": "elrFDKMSiWQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8szQR2ClFv1j"
      },
      "source": [
        "Now, let's ask a random question related to neuroradiology and observe its response. Given that we are utilizing the instruct model to achieve optimal performance, it's essential that we adhere to the 'role':'content' format for our messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_uDWFZFxgNt"
      },
      "outputs": [],
      "source": [
        "# Simple LLM inference with HuggingFace\n",
        "\n",
        "question = \"Where VNS devices are usually implanted?\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an expert neuroradiologist, and what to answer some questions regarding Compatibility of Standard Vagus Nerve Stimulation and Investigational Microburst Vagus Nerve Stimulation Therapy with fMRI\"},\n",
        "    {\"role\": \"user\", \"content\": f\"\"},\n",
        "]\n",
        "\n",
        "llm_response = qa_with_hf_llms(messages, model, tokenizer)\n",
        "\n",
        "print(f'Prompt:\\n```{messages}```')\n",
        "print(f'\\nLLM response:\\n```{llm_response}```')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pTl436aGXOt"
      },
      "source": [
        "As expected, the model provided us with some valid answers. However, this answer is based on its general knowledge learned during pretraining. The Llama 3 model does not have any access to our documents yet. However, this answer is promising so far..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCjG5d_MCoSe"
      },
      "source": [
        "## **Part 1: Retrieval Augmented Generation (RAG)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okOiWx9mI6Tl"
      },
      "source": [
        "### Simplified Overview of RAG\n",
        "\n",
        "The RAG framework offers a blend of traditional large language models and external knowledge retrieval, making it especially beneficial for specialized tasks. Let's simplify the process with a general overview without diving into a specific domain.\n",
        "\n",
        "Imagine a vast digital library filled with books on a variety of subjects. Now, think of the RAG system as a librarian with an impeccable memory. When you ask this librarian a question, rather than relying solely on memory, they looks up relevant information from the library to provide a comprehensive and precise answer.\n",
        "\n",
        "Here’s a step-by-step breakdown of the process:\n",
        "\n",
        "1.  **Embedding Knowledge**: Initially, the RAG system scans all the books (or documents) in the library, understanding their content, and converting each page into a digital fingerprint, or \"vector\". These vectors are stored in a special digital catalog.\n",
        "\n",
        "2.  **Question Analysis**: Now, when you ask the librarian a question, they instantly translate your question into a similar digital fingerprint to know what to look for in the catalog.\n",
        "\n",
        "3.  **Finding Relevant Information**: Using the fingerprint of your question, the librarian quickly searches the catalog to find the pages (or chunks of data) most closely related to your query, as if comparing the similarities between the patterns of two fingerprints.\n",
        "\n",
        "4.  **Crafting the Response**: With the relevant pages in hand, the librarian now composes a well-informed answer, ensuring it's based on the information from the library. This answer is not just from memory but is augmented by the recent information they retrieved.\n",
        "\n",
        "At the heart of this process is the digital catalog (vector database). It ensures that the RAG system provides answers grounded in the information it has been provided, ensuring accuracy and relevance. This approach is particularly beneficial for scenarios where a system needs to tap into specific, up-to-date, or domain-relevant data to answer queries effectively.\n",
        "\n",
        "The following figure simplifies the above methodology for question answering with LLMs using the RAG methodology:\n",
        "<img src=\"https://i.ibb.co/5GchbqR/RAG.jpg\" alt=\"RAG\" border=\"0\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rXShH6gKQ3l"
      },
      "source": [
        "### Chunking the texts\n",
        "\n",
        "To facilitate efficient document retrieval, especially when dealing with large text files, it's often beneficial to divide these documents into manageable \"**chunks**\". This allows for faster indexing, storage, and retrieval, which is paramount in real-time applications like RAG.\n",
        "\n",
        "In the next cell, you will find a function that receives a list of text documents, and returns another listing, consisting of chunks of those documents. As you see, it can also split every text to chunks of certain size with some overlap between the chunks.\n",
        "\n",
        "> **Question**: why do we need to put leave some overlaps between the chunks we are generating?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZzTjCdDc3wQ"
      },
      "outputs": [],
      "source": [
        "# Split the PDF pages into chunks\n",
        "\n",
        "def chunk_text(text, chunk_size=1500, chunk_overlap=200):\n",
        "    chunked_docs = []\n",
        "    i = 0\n",
        "    while i < len(text):\n",
        "        # Determine the end of the current chunk, considering the document's length\n",
        "        end_index = min(i + chunk_size, len(text))\n",
        "        chunk = text[i:end_index]\n",
        "        chunked_docs.append('...'+chunk+'...')\n",
        "\n",
        "        # Advance i to start the next chunk, accounting for overlap\n",
        "        i += chunk_size - chunk_overlap\n",
        "\n",
        "        # Avoid creating a tiny chunk at the end by breaking if the next\n",
        "        # start is too close to the document's end\n",
        "        if i + chunk_size - chunk_overlap > len(text):\n",
        "            break\n",
        "\n",
        "    return chunked_docs\n",
        "\n",
        "chunked_docs = list()\n",
        "for pdf_doc in pdf_docs:\n",
        "  for i, chunk in enumerate(chunk_text(pdf_doc['page_content'])):\n",
        "    chunked_docs.append({\n",
        "        \"source\": pdf_doc['source'],\n",
        "        \"page\": pdf_doc['page'],\n",
        "        \"chunk\": chunk,\n",
        "        \"chunk_index\": i\n",
        "    })\n",
        "\n",
        "print(f'Number of chunks: {len(chunked_docs)}')\n",
        "print(f'One sample chunk: {chunked_docs[100][\"chunk\"]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOupVW_RKjo9"
      },
      "source": [
        "### Setting Up the Embedding Model\n",
        "\n",
        "Embeddings play a pivotal role in retrieval tasks. They transform our textual data into numerical vectors in a high-dimensional space, where semantically similar documents are closer to each other. This allows for efficient searching and matching of related content. The next cell defines a free embedding model from HuggingFace. Alternatively, you could use the OpenAI interface for embedding as well. The embeddings from OpenAI are larger, and often, semantically richer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEmvjwFweEw9"
      },
      "outputs": [],
      "source": [
        "# Setup the embedding model\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "class MyEmbeddingFunction():\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        batch_size=32,\n",
        "        normalize_embeddings=True,\n",
        "        device=\"cuda\"\n",
        "    ):\n",
        "        self.model_id = model_id\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "        self.normalize_embeddings = normalize_embeddings\n",
        "\n",
        "    def __call__(self, input):\n",
        "        embed_model = SentenceTransformer(\n",
        "          model_name_or_path=self.model_id,\n",
        "          device=self.device,\n",
        "        )\n",
        "        embeddings = embed_model.encode(\n",
        "            input,\n",
        "            batch_size=self.batch_size,\n",
        "            normalize_embeddings=self.normalize_embeddings\n",
        "        ).tolist()\n",
        "        return embeddings\n",
        "\n",
        "embed_fn = MyEmbeddingFunction()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmRL0x6ZLOnW"
      },
      "source": [
        "Before diving deep into RAG with large datasets, it's always good to ensure that our embedding model works as expected. This segment provides a demonstration using a simple list of sample texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FLUiUTtl-AI"
      },
      "outputs": [],
      "source": [
        "# Demonstrate the embed_model performance\n",
        "\n",
        "sample_texts = [\n",
        "    'This is sample text 1.',\n",
        "    'This is sample text 2.',\n",
        "    'This is sample text 3.',\n",
        "    'This is sample text 4.',\n",
        "    'This is sample text 5.',\n",
        "]\n",
        "\n",
        "embeddings = embed_fn(sample_texts)\n",
        "\n",
        "print(f\"We have {len(embeddings)} doc embeddings, each with \"\n",
        "      f\"a dimensionality of {len(embeddings[0])}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pf-bsUCLzSY"
      },
      "source": [
        "For retrieval tasks, it's not just enough to create embeddings; we also need an efficient storage and retrieval system for these vector representations. In this section, we set up a vector store using LangChain's plugin for the \"**ChromaDB**\" vector store and populate it with our document embeddings.\n",
        "\n",
        "ChromaDB is an open-source vector store used for storing and retrieving vector embeddings. It is a Python library that helps us work with vector stores, basically a vector database. With ChromaDB, we can store vector embeddings, perform semantic searches, similarity searches, and retrieve vector embeddings. It is designed to save embeddings along with metadata to be used later by large language models1. Additionally, it can also be used for semantic search engines over text data.\n",
        "\n",
        "In the code below, we create a vector database using ChromaDB and save the embeddings of our current chunks into that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3oO7LbThQb6"
      },
      "outputs": [],
      "source": [
        "# Setup a vector store and load it with all vector embeddings\n",
        "\n",
        "import chromadb\n",
        "\n",
        "# Make sure we do not overwrite a previous collection that can cause memory issues\n",
        "try:\n",
        "  print(f\"The vector_db already exists with {vector_db.count()} records!\")\n",
        "except NameError:\n",
        "  # Build an empty Chroma collection\n",
        "  chroma_client = chromadb.PersistentClient(path='./chroma_vectors')\n",
        "  vector_db = chroma_client.get_or_create_collection(\n",
        "      name=\"rag_collection\",\n",
        "      metadata={\"hnsw:space\": \"cosine\"},\n",
        "      embedding_function=embed_fn,\n",
        "  )\n",
        "\n",
        "  # Add the chunks to the collection and let it embed them for further retrieval\n",
        "  vector_db.add(\n",
        "      documents = [chunked_doc['chunk'] for chunked_doc in chunked_docs],\n",
        "      metadatas = [\n",
        "          {\n",
        "            \"type\": \"article_chunk\",\n",
        "            \"source\": chunked_doc[\"source\"],\n",
        "            \"page\": chunked_doc[\"page\"],\n",
        "            \"chunk_index\": chunked_doc[\"chunk_index\"],\n",
        "          }\n",
        "      for chunked_doc in chunked_docs],\n",
        "      ids = [str(i) for i in range(len(chunked_docs))]\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NV04Q6RMdYe"
      },
      "source": [
        "Now that our vector store is populated with embeddings, let's demonstrate the retrieval process. The idea is to query the vector store to find the most semantically relevant document chunks based on our query. But how can we do that?\n",
        "\n",
        "**The magic of cosine similarity**:\n",
        "\n",
        "Cosine similarity is a metric that measures the cosine of the angle between two vectors. It is often used to compute the similarity between word vectors, indicating how similar two words are in terms of their usage or meaning. In the context of Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs), cosine similarity can be used to compute the similarity between embeddings or vectors representing different pieces of text. By comparing the cosine similarity between these embeddings, we can identify how similar or related they are to each other.\n",
        "\n",
        "So what happens behind the scene when we query our vector database is that the cosine similarity between our query vector and all stored vectors in the database will be computed, and those with maximum similarity will be returned. These vectors belong to text chunks that are most likely to be similar - in terms of content - to our queried question.\n",
        "\n",
        "Let's demonstrate this process below:\n",
        "\n",
        ">**Question**: Look at the returned chunks. Do they look relevant to the question we asked?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sx8Z9B1kX_L"
      },
      "outputs": [],
      "source": [
        "# Demonstrating how the retriever works\n",
        "\n",
        "outputs = vector_db.query(\n",
        "    query_texts=question,  # the search query\n",
        "    n_results=3  # returns top 3 most relevant chunks of text\n",
        ")\n",
        "\n",
        "# Let's see what is available in the returned `outputs` object\n",
        "\n",
        "print(outputs.keys())\n",
        "\n",
        "# And then print the retrieved chunks and their cosine distances with respect to the queried question.\n",
        "# Chromadb refers to returned chunks as `documents`.\n",
        "\n",
        "for i, (doc, dist,meta) in enumerate(zip(outputs['documents'][0], outputs['distances'][0], outputs['metadatas'][0])):\n",
        "    print(f'item: {i+1} - distance: {dist}\\nText: {doc}\\n\\nMetadata: {meta}\\n\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7XcF-uRNUCf"
      },
      "source": [
        "### Setting up a RAG pipeline\n",
        "\n",
        "Now that we have our vector database set up, let's put together a RAG pipeline using Chroma and Mistral. Note that this pipeline works like the generator pipeline we created above, but it is guaranteed to work based on RAG; which means whatever responses the model generates is going to be grounded in some chunks of texts that have been extracted from the vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T61AVKqvklD5"
      },
      "outputs": [],
      "source": [
        "def do_rag(\n",
        "    question,\n",
        "    vector_db,\n",
        "    num_retrieval=3,\n",
        "    return_retrieved_chunks=True\n",
        "):\n",
        "\n",
        "  # Do the retrieval\n",
        "  outputs = vector_db.query(\n",
        "    query_texts=question,  # the search query\n",
        "    n_results=num_retrieval  # returns top 3 most relevant chunks of text\n",
        "  )\n",
        "  retrieved_chunks = outputs['documents'][0]\n",
        "\n",
        "  # Merge the retrieved documents to build a `context` string\n",
        "  context = \"\\n\\n\".join(retrieved_chunks)\n",
        "\n",
        "  # Build a prompt\n",
        "  messages = [\n",
        "    {\"role\": \"system\", \"content\": f\"Answer the question based on the provided context. Only rely onthe context to build your answer and do not use your own knowledge: {context}\"},\n",
        "    {\"role\": \"user\", \"content\": f\"{question}\"},\n",
        "]\n",
        "  # Ask the prompt from the language model\n",
        "  llm_response = qa_with_hf_llms(messages, model, tokenizer)\n",
        "  if return_retrieved_chunks:\n",
        "    return llm_response, context\n",
        "  return llm_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcftkHTf-_Co"
      },
      "outputs": [],
      "source": [
        "# Let's check the RAG pipeline we just set up:\n",
        "\n",
        "question = \"What are the brain locations that model is looking at for CSF venous fistula'prediction?\"\n",
        "llm_response, context = do_rag(question, vector_db)\n",
        "\n",
        "print(f'Here is the retrieved context:\\n{context}\\n\\n')\n",
        "print(f'Here is the LLM response: {llm_response}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGGZSSDYRSKQ"
      },
      "source": [
        "This brings us to the end of this notebook. Thank you for reading our code. We hope you have found it useful!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}